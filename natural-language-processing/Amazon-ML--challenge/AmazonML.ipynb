{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1034,
     "status": "ok",
     "timestamp": 1627652844159,
     "user": {
      "displayName": "Shaury Srivastava",
      "photoUrl": "",
      "userId": "17221522711714923321"
     },
     "user_tz": -330
    },
    "id": "1XGVFakW8Ptm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import nltk\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing the necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 98258,
     "status": "ok",
     "timestamp": 1627652950119,
     "user": {
      "displayName": "Shaury Srivastava",
      "photoUrl": "",
      "userId": "17221522711714923321"
     },
     "user_tz": -330
    },
    "id": "ylhxj0rPB7e6"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/AMAZON/dataset/train.csv',escapechar = \"\\\\\",quoting = csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "executionInfo": {
     "elapsed": 2040,
     "status": "ok",
     "timestamp": 1627652967195,
     "user": {
      "displayName": "Shaury Srivastava",
      "photoUrl": "",
      "userId": "17221522711714923321"
     },
     "user_tz": -330
    },
    "id": "QQ-8rHEt9FWN",
    "outputId": "62066146-ba6a-4501-d475-42e145c1acd9"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## checking the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "executionInfo": {
     "elapsed": 24725,
     "status": "ok",
     "timestamp": 1627652995891,
     "user": {
      "displayName": "Shaury Srivastava",
      "photoUrl": "",
      "userId": "17221522711714923321"
     },
     "user_tz": -330
    },
    "id": "YBgtL9lYHCpx",
    "outputId": "c2be7f03-edbd-439b-d51f-0805cb07cefe"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(df.isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imputing the title values in the missing bullet points columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1627652995894,
     "user": {
      "displayName": "Shaury Srivastava",
      "photoUrl": "",
      "userId": "17221522711714923321"
     },
     "user_tz": -330
    },
    "id": "2dcaeniJ7KaQ"
   },
   "outputs": [],
   "source": [
    "df.loc[df['BULLET_POINTS'].isna(),'BULLET_POINTS'] = df[df['BULLET_POINTS'].isna()]['TITLE'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dropping the description and brands columns due to presence of many missing values and no correlation between the taget column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1627652995896,
     "user": {
      "displayName": "Shaury Srivastava",
      "photoUrl": "",
      "userId": "17221522711714923321"
     },
     "user_tz": -330
    },
    "id": "1Gj0Y189-zVr",
    "outputId": "27fbecb8-8204-4e50-d13f-edd61fa8c93f"
   },
   "outputs": [],
   "source": [
    "df = df.drop(['DESCRIPTION','BRAND'],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1627652995898,
     "user": {
      "displayName": "Shaury Srivastava",
      "photoUrl": "",
      "userId": "17221522711714923321"
     },
     "user_tz": -330
    },
    "id": "DJ-aTZv17SZM"
   },
   "outputs": [],
   "source": [
    "spec_chars = [\"!\",'\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\",\n",
    "              \"*\",\"+\",\",\",\"-\",\".\",\"/\",\":\",\";\",\"<\",\n",
    "              \"=\",\">\",\"?\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\n",
    "              \"`\",\"{\",\"|\",\"}\",\"~\",\"â€“\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## removing the special characters from the dataframe strings from the columns bullet_points and title "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOcFBTkdrJN4"
   },
   "source": [
    "TITLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "executionInfo": {
     "elapsed": 102925,
     "status": "ok",
     "timestamp": 1627653103245,
     "user": {
      "displayName": "Shaury Srivastava",
      "photoUrl": "",
      "userId": "17221522711714923321"
     },
     "user_tz": -330
    },
    "id": "r95alEnvEEoc",
    "outputId": "f0a8ceef-165e-4ec6-821e-23f40ce23f3b"
   },
   "outputs": [],
   "source": [
    "df['title'] = df['TITLE'].str.lower()\n",
    "df['title'] = df['title'].str.replace('\\d+', '')\n",
    "for char in spec_chars:\n",
    "    df['title'] = df['title'].str.replace(char, ' ')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l71laEz-rDq-"
   },
   "source": [
    "## BULLET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "executionInfo": {
     "elapsed": 161615,
     "status": "ok",
     "timestamp": 1627653264852,
     "user": {
      "displayName": "Shaury Srivastava",
      "photoUrl": "",
      "userId": "17221522711714923321"
     },
     "user_tz": -330
    },
    "id": "o2_KF2_GEFI0",
    "outputId": "9d4e12b1-6f77-49da-92ef-2cb3a4c919a1"
   },
   "outputs": [],
   "source": [
    "df['bullet'] = df['BULLET_POINTS'].str.lower()\n",
    "df['bullet'] = df['bullet'].str.replace('\\d+', '')\n",
    "for char in spec_chars:\n",
    "    df['bullet'] = df['bullet'].str.replace(char, ' ')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## droping the original title and bullet columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1627653264855,
     "user": {
      "displayName": "Shaury Srivastava",
      "photoUrl": "",
      "userId": "17221522711714923321"
     },
     "user_tz": -330
    },
    "id": "135l0wEfEFo9"
   },
   "outputs": [],
   "source": [
    "df.drop(['TITLE','BULLET_POINTS'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtmBIpLltu0m"
   },
   "source": [
    "## STOP WORDS REMOVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df['title'] + \" \" + df['bullet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 846,
     "status": "ok",
     "timestamp": 1627653265687,
     "user": {
      "displayName": "Shaury Srivastava",
      "photoUrl": "",
      "userId": "17221522711714923321"
     },
     "user_tz": -330
    },
    "id": "EhYnCOlOBW1H"
   },
   "outputs": [],
   "source": [
    "en = spacy.load('en_core_web_sm')\n",
    "sw_spacy = en.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1627653265689,
     "user": {
      "displayName": "Shaury Srivastava",
      "photoUrl": "",
      "userId": "17221522711714923321"
     },
     "user_tz": -330
    },
    "id": "ns9XUo0YA8dI"
   },
   "outputs": [],
   "source": [
    "def fun(text):\n",
    "    if(type(text)==str):\n",
    "        words = [word for word in text.split() if word not in sw_spacy]\n",
    "        new_text = \" \".join(words)\n",
    "        return new_text\n",
    "    else: \n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11776,
     "status": "ok",
     "timestamp": 1627653277453,
     "user": {
      "displayName": "Shaury Srivastava",
      "photoUrl": "",
      "userId": "17221522711714923321"
     },
     "user_tz": -330
    },
    "id": "6O_oHEKrEFzK"
   },
   "outputs": [],
   "source": [
    "df['title'] = df['title'].astype('object').map(fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1627653533855,
     "user": {
      "displayName": "Shaury Srivastava",
      "photoUrl": "",
      "userId": "17221522711714923321"
     },
     "user_tz": -330
    },
    "id": "oZv-wub5td2A",
    "outputId": "ed572391-a3e5-4474-8952-c5b80196f2de"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imputing nan titles with the word 'cover case' due to its high presence in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['title'].isna(),'title'] = 'cover case'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## removing the duplicate words from the dataset rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uni(x):\n",
    "    l = np.unique(x.split())\n",
    "    return \" \".join(l)\n",
    "test['title'] = test['title'].map(uni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## removing the non english words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "words = set(nltk.corpus.words.words())\n",
    "def fun1(text):\n",
    "    if(type(text)==str):\n",
    "        words1 = [word for word in text.split() if word  in words]\n",
    "        new_text = \" \".join(words1)\n",
    "        return new_text\n",
    "    else:\n",
    "        return text\n",
    "df['title'] = df['title'].map(fun1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 745,
     "status": "ok",
     "timestamp": 1627655180565,
     "user": {
      "displayName": "Shaury Srivastava",
      "photoUrl": "",
      "userId": "17221522711714923321"
     },
     "user_tz": -330
    },
    "id": "FIvj-xBxDoJO"
   },
   "outputs": [],
   "source": [
    "def len_fun(text):\n",
    "    if(type(text)==str):\n",
    "        words = text.split()\n",
    "        ans = len(words)\n",
    "    return ans\n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4538,
     "status": "ok",
     "timestamp": 1627655189093,
     "user": {
      "displayName": "Shaury Srivastava",
      "photoUrl": "",
      "userId": "17221522711714923321"
     },
     "user_tz": -330
    },
    "id": "uyg5rxpR7RoU"
   },
   "outputs": [],
   "source": [
    "df['len_title'] = df['title'].map(len_fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot for the count of words in the dataset in the title column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572
    },
    "executionInfo": {
     "elapsed": 3118,
     "status": "ok",
     "timestamp": 1627657325144,
     "user": {
      "displayName": "Shaury Srivastava",
      "photoUrl": "",
      "userId": "17221522711714923321"
     },
     "user_tz": -330
    },
    "id": "C1zPnZx4N4yT",
    "outputId": "6affc980-944b-41be-fb73-1f8cbe81c1da"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.countplot(x=df['len_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1627655277376,
     "user": {
      "displayName": "Shaury Srivastava",
      "photoUrl": "",
      "userId": "17221522711714923321"
     },
     "user_tz": -330
    },
    "id": "ZxK4kyQ4fpmQ"
   },
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)\n",
    "df.drop('index',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nho_-Ap61PIV"
   },
   "source": [
    "## STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1579694,
     "status": "ok",
     "timestamp": 1627657003795,
     "user": {
      "displayName": "Shaury Srivastava",
      "photoUrl": "",
      "userId": "17221522711714923321"
     },
     "user_tz": -330
    },
    "id": "aP9QfUGF0kOk"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "  \n",
    "ps = PorterStemmer()\n",
    " \n",
    "def stm(text):\n",
    "    if(type(text)==str):\n",
    "        words = text.split()\n",
    "        new_text =\"\"\n",
    "        for w in words:\n",
    "            new_text = new_text + \" \"+ ps.stem(w)\n",
    "        return new_text\n",
    "    else: \n",
    "        return text\n",
    "\n",
    "df['title'] = df['title'].map(stm) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## making a dataset with 10 samples of each browse_node_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df[df['BROWSE_NODE_ID']==0].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df[df['BROWSE_NODE_ID']==0],df[df['BROWSE_NODE_ID']==1]],axis=0).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()\n",
    "for i in pd.unique(df['BROWSE_NODE_ID']):\n",
    "    if(i!=0):\n",
    "        if(df[df['BROWSE_NODE_ID']==i]['BROWSE_NODE_ID'].count()>10):\n",
    "            df1 = pd.concat([df1,df[df['BROWSE_NODE_ID']==i].head(10)],axis=0)\n",
    "        else:\n",
    "            df1 = pd.concat([df1,df[df['BROWSE_NODE_ID']==i]],axis=0)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## converting the dataset into vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "df.dropna(axis=0,inplace=True)\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "## applying the train test split for validation\n",
    "#xtrain, xval, ytrain, yval = train_test_split(df['title'], df['BROWSE_NODE_ID'], train_size=0.8, random_state=101)\n",
    "\n",
    "xtrain_tfidf = tfidf_vectorizer.fit_transform(df['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## making the sparse matrix small by using Principal component analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "sparse_dataset = csr_matrix(xtrain_tfidf)\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy import sparse as sp\n",
    "tsvd = TruncatedSVD(n_components=500)\n",
    "x_train_tsvd = tsvd.fit(sparse_dataset).transform(sparse_dataset)\n",
    "\n",
    "sparse_datasetval = csr_matrix(xval_tfidf)\n",
    "x_val_tsvd = tsvd.fit(sparse_dataset).transform(sparse_datasetval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modeling using the naive bayes method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(x_train_tsvd,ytrain)\n",
    "\n",
    "prd = model.predict(x_val_tsvd)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(prd,yval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## due to low accuracy employing the full data as train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_tfidf = tfidf_vectorizer.fit_transform(xtrain)\n",
    "from scipy.sparse import csr_matrix\n",
    "sparse_dataset = csr_matrix(xtrain_tfidf)\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy import sparse as sp\n",
    "tsvd = TruncatedSVD(n_components=5000)\n",
    "xfit = tsvd.fit(sparse_dataset)\n",
    "x_train_tsvd = xfit.transform(sparse_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## applying the same methods for test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/AMAZON/dataset/test.csv',escapechar = \"\\\\\",quoting = csv.QUOTE_NONE)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imputing the title values in the missing bullet points columns\n",
    "\n",
    "test.loc[test['BULLET_POINTS'].isna(),'BULLET_POINTS'] = test[test['BULLET_POINTS'].isna()]['TITLE'] \n",
    "\n",
    "# dropping the description and brands columns due to presence of many missing values and no correlation between the taget column.\n",
    "\n",
    "test = test.drop(['DESCRIPTION','BRAND'],axis=1)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['title'] = test['TITLE'].str.lower()\n",
    "test['title'] = test['title'].str.replace('\\d+', '')\n",
    "for char in spec_chars:\n",
    "    test['title'] = test['title'].str.replace(char, ' ')\n",
    "\n",
    "\n",
    "## BULLET\n",
    "\n",
    "test['bullet'] = test['BULLET_POINTS'].str.lower()\n",
    "test['bullet'] = test['bullet'].str.replace('\\d+', '')\n",
    "for char in spec_chars:\n",
    "    test['bullet'] = test['bullet'].str.replace(char, ' ')\n",
    "\n",
    "\n",
    "## droping the original title and bullet columns \n",
    "\n",
    "test.drop(['TITLE','BULLET_POINTS'],axis=1,inplace=True)\n",
    "\n",
    "## STOP WORDS REMOVAL\n",
    "\n",
    "test['title'] = test['title'] + \" \" + test['bullet']\n",
    "test.drop('bullet',axis=1,inplace=True)\n",
    "test['title'] = test['title'].astype('object').map(fun)\n",
    "\n",
    "\n",
    "\n",
    "## imputing nan titles with the word 'cover case' due to its high presence in the dataset\n",
    "\n",
    "test.loc[test['title'].isna(),'title'] = 'cover case'\n",
    "\n",
    "## removing the duplicate words from the dataset rows\n",
    "test['title'] = test['title'].map(uni)\n",
    "\n",
    "## removing the non english words\n",
    "test['title'] = test['title'].map(fun1)\n",
    "\n",
    "test.reset_index(inplace=True)\n",
    "test.drop('index',axis=1,inplace=True)\n",
    "\n",
    "## STEMMING\n",
    "test['title'] = test['title'].map(stm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xval= test['title']\n",
    "xval1 = xval[0:50000]\n",
    "xval2 = xval[50000:len(xval)]\n",
    "xval_tfidf = tfidf_vectorizer.transform(xval1)\n",
    "sparse_datasetval = csr_matrix(xval_tfidf)\n",
    "x_val_tsvd = xfit.transform(sparse_datasetval)\n",
    "\n",
    "prd = model.predict(x_val_tsvd)\n",
    "\n",
    "xval_tfidf = tfidf_vectorizer.transform(xval2)\n",
    "sparse_datasetval = csr_matrix(xval_tfidf)\n",
    "x_val_tsvd = xfit.transform(sparse_datasetval)\n",
    "\n",
    "prd1 = model.predict(x_val_tsvd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.concat([prd,prd1],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.concat([pred,test['PRODUCT_ID']],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.to_csv('/ans.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AmazonML.ipynb",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
