x='abc$#@ ABC09'
toSpace(x)
[:alnum:]
toSpace = function(x){
vec<- gsub('[^[:alnum:][:space:]]','',vec)
vec<- gsub('[0-9]','',vec)
return (vec)
}
x='abc$#@ ABC09'
toSpace(x)
toSpace = function(x){
vec<- gsub('[[:alnum:][:space:]]','',vec)
vec<- gsub('[0-9]','',vec)
return (vec)
}
x='abc$#@ ABC09'
toSpace(x)
toSpace = function(x){
vec<- gsub('[^[:alnum:][:space:]]','',vec)
vec<- gsub('[0-9]','',vec)
return (vec)
}
x='abc$#@ ABC09'
toSpace(x)
toSpace = function(x){
vec<- gsub('[^[:alnum:][:space:]]','',vec)
#vec<- gsub('[0-9]','',vec)
return (vec)
}
x='abc$#@ ABC09'
toSpace(x)
toSpace = function(x){
vec<- gsub('[^[:alnum:][:space:]]','',vec)
#vec<- gsub('[0-9]','',vec)
return (vec)
}
x='abc$#@ ABC09'
toSpace(x)
toSpace = function(x){
vec<- gsub('[^[:alnum:][:space:]]','',x)
#vec<- gsub('[0-9]','',vec)
return (vec)
}
x='abc$#@ ABC09'
toSpace(x)
toSpace = function(x){
vec<- gsub('[^[:alnum:][:space:]]','',x)
vec<- gsub('[0-9]','',vec)
return (vec)
}
x='abc$#@ ABC09'
toSpace(x)
covid.corpus<- Corpus(VectorSource(covid.tweets));
toSpace = function(x){
vec<- gsub('[^[:alnum:][:space:]]','',x)
vec<- gsub('[0-9]','',vec)
return (vec)
}
x='abc$#@ ABC09'
toSpace(x)
covid.corpus<-sapply(covid.corpus,toSpace)
covid.corpus
toSpace = function(x){
vec<- gsub('[^[:alphabet:][:space:]]','',x)
#vec<- gsub('[0-9]','',vec)
return (vec)
}
x='abc$#@ ABC09'
toSpace(x)
toSpace = function(x){
vec<- gsub('[^[:alpha:][:space:]]','',x)
#vec<- gsub('[0-9]','',vec)
return (vec)
}
x='abc$#@ ABC09'
toSpace(x)
toSpace = function(x){
return (gsub('[^[:alpha:][:space:]]','',x))
}
x='abc$#@ ABC09'
toSpace(x)
covid.corpus<- Corpus(VectorSource(covid.tweets));
toSpace = function(x){
return (gsub('[^[:alpha:][:space:]]','',x))
}
covid.corpus<-sapply(covid.corpus,toSpace)
covid.corpus<-tm_map(covid.corpus,content_transformer(tolower))
covid.corpus<- Corpus(VectorSource(covid.tweets));
covid.corpus<-tm_map(covid.corpus,toSpace)
covid.corpus<- Corpus(VectorSource(covid.tweets));
toSpace = function(x){
return (gsub('[^[:alpha:][:space:]]','',x))
}
covid.corpus<-tapply(covid.corpus,toSpace)
covid.corpus<-lapply(covid.corpus,toSpace)
covid.corpus<-tm_map(covid.corpus,content_transformer(tolower))
covid.corpus<- Corpus(VectorSource(covid.tweets));
tm_map
covid.corpus<-lapply(covid.corpus,toSpace)
covid.corpus<-Corpus(VectorSource(covid.corpus))
covid.corpus<-tm_map(covid.corpus,content_transformer(tolower))
covid.corpus[1]
covid.corpus<- Corpus(VectorSource(covid.tweets));
toSpace = function(x){
return (gsub('[^[:alpha:][:space:]]','',x))
}
covid.corpus<-lapply(covid.corpus,toSpace)
covid.corpus<-Corpus(VectorSource(covid.corpus))
covid.text <- sapply(covid.tweets,function(x) x$getText());
covid.corpus<- Corpus(VectorSource(covid.text));
covid.text <- sapply(covid.tweets,function(x) x$getText());
covid.text<- iconv(covid.text,'UTF-8','ASCII');
covid.corpus<- Corpus(VectorSource(covid.text));
term.doc.matrix = TermDocumentMatrix(covid.corpus,control=list(removePunctuation = TRUE,tolower=TRUE,stopwords= c('lockdown',stopwords('english'),'covid','covid19','virus'),removeNumbers=TRUE));
head(term.doc.matrix)
term.doc.matrix<- as.matrix(term.doc.matrix)
dm<- sort(rowSums(term.doc.matrix),decreasing = TRUE);
dm<-data.frame(word = names(dm),freq=dm)
head(dm)
covid.tweets <- searchTwitter('covid19',n=1000,lang=NULL, since = '2021-06-24', until = '2021-06-25')
covid.text <- sapply(covid.tweets,function(x) x$getText());
covid.text<- iconv(covid.text,'UTF-8','ASCII');
covid.corpus<- Corpus(VectorSource(covid.text));
term.doc.matrix = TermDocumentMatrix(covid.corpus,control=list(removePunctuation = TRUE,tolower=TRUE,stopwords= c('lockdown',stopwords('english'),'covid','covid19','virus'),removeNumbers=TRUE));
head(term.doc.matrix)
term.doc.matrix<- as.matrix(term.doc.matrix)
dm<- sort(rowSums(term.doc.matrix),decreasing = TRUE);
dm<-data.frame(word = names(dm),freq=dm)
head(dm)
nrow(dm)
tail(dm)
library(tidyverse)
ggplot(dm)+geom_polygon(aes(freq))
ggplot(dm)+geom_polygon(aes(freq,word))
ggplot(dm)+geom_polygon(aes(word,freq))
ggplot(dm[50,])+geom_polygon(aes(word,freq))
ggplot(dm[50,])+geom_histogram(aes(freq))
ggplot(dm[5,])+geom_histogram(aes(freq))
ggplot(dm[5,])+geom_histogram(aes(freq),dodge='word')
ggplot(dm[5,])+geom_histogram(aes(freq),position ='dodge')
ggplot(dm[5,])+geom_histogram(aes(freq,color=factor(word)),position ='dodge')
ggplot(dm[5,])+geom_histogram(aes(freq,fill=factor(word)),position ='dodge')
ggplot(dm[1:5,])+geom_histogram(aes(freq,fill=factor(word)),position ='dodge')
ggplot(dm[1:5,])+geom_histogram(aes(y=freq,fill=factor(word)),position ='dodge')
ggplot(dm[1:5,])+geom_bar(aes(y=freq,fill=factor(word)),position ='dodge')
ggplot(dm[1:5,])+geom_bar(aes(freq,fill=factor(word)),position ='dodge')
ggplot(dm[1:5,])+geom_bar(aes(x=word,y=freq,fill=factor(word)),position ='dodge')
ggplot(dm[1:5,])+geom_histogram(aes(x=word,y=freq,fill=factor(word)),position ='dodge')
ggplot(dm[1:5,])+geom_line(aes(x=word,y=freq,fill=factor(word)),position ='dodge')
ggplot(dm)+geom_line(aes(freq))
rownames(dm)<-NULL;
head(dm)
ggplot(dm)+geom_line(aes(index,freq))
id<- seq(1:nrow(dm));
dm<-cbind(id,dm)
ggplot(dm)+geom_line(aes(id,freq))
ggplot(dm)+geom_bar(aes(freq))
barplot(dm$freq,names.arg = dm$word)
barplot(dm[5,]$freq,names.arg = dm[5,]$word)
barplot(dm[1:5,]$freq,names.arg = dm[1:5,]$word)
wordcloud(words = dm$word, freq = dm$freq, min.freq = 5,
max.words=100, random.order=FALSE, rot.per=0.40,
colors=brewer.pal(8, "Dark2"))
term.doc.matrix = TermDocumentMatrix(covid.corpus,control=list(removePunctuation = TRUE,tolower=TRUE,stopwords= c('lockdown',stopwords('english'),'covid','covid19','virus','ocgn'),removeNumbers=TRUE));
head(term.doc.matrix)
term.doc.matrix<- as.matrix(term.doc.matrix)
dm<- sort(rowSums(term.doc.matrix),decreasing = TRUE);
rownames(dm)<-NULL;
dm<-data.frame(word = names(dm),freq=dm)
id<- seq(1:nrow(dm));
dm<-cbind(id,dm)
head(dm)
nrow(dm)
tail(dm)
library(tidyverse)
barplot(dm[1:5,]$freq,names.arg = dm[1:5,]$word)
ggplot(dm)+geom_bar(aes(freq))
set.seed(1234)
wordcloud(words = dm$word, freq = dm$freq, min.freq = 5,
max.words=100, random.order=FALSE, rot.per=0.40,
colors=brewer.pal(8, "Dark2"))
barplot(dm[1:5,]$freq,names.arg = dm[1:5,]$word)
ggplot(dm)+geom_bar(aes(freq))
ggplot(dm[1:5,])+geom_histogram(aes(x=word,y=freq))
ggplot(dm[1:5,])+geom_histogram(aes(y=freq))
ggplot(dm[1:5,])+geom_histogram(aes(x=word,count=freq))
ggplot(dm[1:5,])+geom_histogram(aes(x=word))
ggplot(dm[1:5,],aes(x=word,y=freq))+geom_histogram()
ggplot(dm[1:5,],aes(x=word,y=freq))+geom_bar(stat = 'identity')
ggplot(dm[1:5,],aes(x=word,y=freq))+geom_bar(stat = 'identity',fill=word)
ggplot(dm[1:5,],aes(x=word,y=freq))+geom_bar(stat = 'identity',fill=factor(word))
ggplot(dm[1:5,],aes(x=word,y=freq))+geom_bar(stat = 'identity',aes(fill=factor(word)))
term.doc.matrix = TermDocumentMatrix(covid.corpus,control=list(removePunctuation = TRUE,tolower=TRUE,stopwords= c('lockdown',stopwords('english'),'covid','covid19','virus','ocgn','variants','variantes','vaccine'),removeNumbers=TRUE));
head(term.doc.matrix)
term.doc.matrix<- as.matrix(term.doc.matrix)
dm<- sort(rowSums(term.doc.matrix),decreasing = TRUE);
rownames(dm)<-NULL;
dm<-data.frame(word = names(dm),freq=dm)
id<- seq(1:nrow(dm));
dm<-cbind(id,dm)
head(dm)
nrow(dm)
tail(dm)
library(tidyverse)
barplot(dm[1:5,]$freq,names.arg = dm[1:5,]$word)
ggplot(dm[1:5,],aes(x=word,y=freq))+geom_bar(stat = 'identity',aes(fill=factor(word)))
ggplot(dm[1:10,],aes(x=word,y=freq))+geom_histogram(stat = 'identity',aes(fill=factor(word)))
covid.tweets <- searchTwitter('covid',n=1000,lang=NULL, since = '2021-06-24', until = '2021-06-25')
covid.text <- sapply(covid.tweets,function(x) x$getText());
covid.text<- iconv(covid.text,'UTF-8','ASCII');
covid.corpus<- Corpus(VectorSource(covid.text));
term.doc.matrix = TermDocumentMatrix(covid.corpus,control=list(removePunctuation = TRUE,tolower=TRUE,stopwords= c('lockdown',stopwords('english'),'covid','covid19','virus','ocgn','variants','variantes','vaccine'),removeNumbers=TRUE));
head(term.doc.matrix)
term.doc.matrix<- as.matrix(term.doc.matrix)
dm<- sort(rowSums(term.doc.matrix),decreasing = TRUE);
rownames(dm)<-NULL;
dm<-data.frame(word = names(dm),freq=dm)
ggplot(dm[1:10,],aes(x=word,y=freq))+geom_histogram(stat = 'identity',aes(fill=factor(word)))
set.seed(1234)
wordcloud(words = dm$word, freq = dm$freq, min.freq = 5,
max.words=100, random.order=FALSE, rot.per=0.40,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = dm$word, freq = dm$freq, min.freq = 5,
max.words=50, random.order=FALSE, rot.per=0.40,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = dm$word, freq = dm$freq, min.freq = 1,
max.words=10, random.order=FALSE, rot.per=0.40,
colors=brewer.pal(8, "Dark2"))
wordcloud(words = dm$word, freq = dm$freq, min.freq = 3,
max.words=10, random.order=FALSE, rot.per=0.40,
colors=brewer.pal(8, "Dark2"))
findAssocs(TextDoc_dtm, terms = c("deaths","variant","delta"), corlimit = 0.25)
findAssocs(dm, terms = c("deaths","variant","delta"), corlimit = 0.25)
findAssocs(covid.text, terms = c("deaths","variant","delta"), corlimit = 0.25)
??findAssocs()
findAssocs(term.doc.matrix, terms = c("deaths","variant","delta"), corlimit = 0.25)
term.doc.matrix = TermDocumentMatrix(covid.corpus,control=list(removePunctuation = TRUE,tolower=TRUE,stopwords= c('lockdown',stopwords('english'),'covid','covid19','virus','ocgn','variants','variantes','vaccine'),removeNumbers=TRUE));
findAssocs(term.doc.matrix, terms = c("deaths","variant","delta"), corlimit = 0.25)
library(syuzhet)
syuzhet_vector <- get_sentiment(covid.text, method="syuzhet")
# see the first row of the vector
head(syuzhet_vector)
# see summary statistics of the vector
summary(syuzhet_vector)
View(syuzhet_vector)
length(syuzhet_vector)
# bing
bing_vector <- get_sentiment(covid.text, method="bing")
head(bing_vector)
summary(bing_vector)
#affin
afinn_vector <- get_sentiment(covid.text, method="afinn")
head(afinn_vector)
summary(afinn_vector)
rbind(
sign(head(syuzhet_vector)),
sign(head(bing_vector)),
sign(head(afinn_vector))
)
#emotions
d<- get_nrc_sentiment(covid.text)
head(d,10)
tail(d,10)
#transpose
td<-data.frame(t(d))
#The function rowSums computes column sums across rows for each level of a grouping variable.
td_new <- data.frame(rowSums(td[2:253]))
#Transformation and cleaning
names(td_new)[1] <- "count"
td_new <- cbind("sentiment" = rownames(td_new), td_new)
rownames(td_new) <- NULL
td_new2<-td_new[1:8,]
#Plot One - count of words associated with each sentiment
quickplot(sentiment, data=td_new2, weight=count, geom="bar", fill=sentiment, ylab="count")+ggtitle("Survey sentiments")
ggplot(td_new2,aes(sentiment,count))+geom_bar(aes(fill=sentiment))
ggplot(td_new2,aes(count))+geom_bar(aes(fill=sentiment))
ggplot(td_new2,aes(count))+geom_bar(aes(fill=sentiment),position = 'dodge')
ggplot(td_new2,aes(x=sentiment,y=count))+geom_bar(aes(fill=sentiment),position = 'dodge')
ggplot(td_new2,aes(x=sentiment,y=count))+geom_histogram(aes(fill=sentiment),position = 'dodge')
ggplot(td_new2,aes(x=sentiment,y=count))+geom_histogram(stat='identity',aes(fill=sentiment),position = 'dodge')
wordcloud(words = dm$word, freq = dm$freq, min.freq = 3,
max.words=10, random.order=FALSE, rot.per=0.40,
colors=brewer.pal(8, "Dark2"))
ggplot(dm[1:10,],aes(x=word,y=freq))+geom_histogram(stat = 'identity',aes(fill=factor(word)))
library(MASS)
head(Boston)
library(tidyverse)
summary(Boston)
any(is.na(Boston))
dataf<-Boston;
ggplot(dataf)+geom_point(aes(crim,medv))
cor(dataf$medv,dataf$crim)
library(corrgram)
corrgram(dataf)
pl+geom_point(aes(blacj
k))
pl+geom_point(aes(black))
pl<- ggplot(dataf,aes(y=medv));
pl+geom_point(aes(black))
pl+geom_point(aes(lstat))
pl+geom_point(aes(tax))
pl+geom_point(aes(rad))
pl+geom_point(aes(age))
cor(dataf)
dataf<- scale(dataf)
cor(dataf)
maxs <- apply(dataf,2,max)
maxs
mins <- apply(dataf,2,min)
mins
??scale
dataf<-Boston;
#normalize data
maxs <- apply(dataf,2,max)
maxs
mins <- apply(dataf,2,min)
mins
scaled.dataf<-scale(dataf,center = mins,scale = maxs-mins);
scaled<- data.frame(dataf)
cor(scaled)
library(caTools)
set.seed(101)
sample<- sample.split(scaled,SplitRatio = 0.7)
train<- subset(scaled,sample==TRUE)
test<- subset(Scaled,sample==FALSE);
train<- subset(scaled,sample==TRUE)
test<- subset(scaled,sample==FALSE);
install.packages('neuralnet')
library(nerualnet)
library(neuralnet)
nn <- neuralnet(medv~(tax+nox+ptratio+lstat+rm+indus),data = train,hidden = c(5,3),linear.output =T)
plot(nn)
nn <- neuralnet(medv~(tax+nox+ptratio+lstat+rm+indus),data = train,hidden = c(6,5),linear.output =T)
plot(nn)
plot(nn)
nn <- neuralnet(medv~(tax+nox+ptratio+lstat+rm+indus),data = train,hidden = c(6,5),linear.output =T)
plot(nn)
pred <- compute(nn,susbet(test,select = -c(medv))
pred <- compute(nn,susbet(test,select = -c(medv))
)
pred <- compute(nn,susbet(test,select = -c(medv)))
pred <- compute(nn,subset(test,select = -c(medv)))
head(pred)
pred$net.result
head(pred$net.result)
str(pred)
true.pred<- pred*(max(dataf$medv)-min(dataf$medv)) + min(dataf$medv)
true.pred<- (pred*(max(dataf$medv)-min(dataf$medv))) + min(dataf$medv)
true.pred<- (pred$net.result*(max(dataf$medv)-min(dataf$medv))) + min(dataf$medv)
unique(pred)
unique(pred$net.result)
nn <- neuralnet(medv~(tax+nox+ptratio+lstat+rm+indus),data = train,hidden = c(6,3),linear.output =T)
plot(nn)
pred <- compute(nn,subset(test,select = -c(medv)))
str(pred)
nn <- neuralnet(medv~tax+nox+ptratio+lstat+rm+indus,data = train,hidden = c(6,3),linear.output =T)
pred <- compute(nn,subset(test,select = -c(medv)))
str(pred)
test.r<- test$medv*(max(dataf$medv)-min(dataf$medv)) + min(dataf$medv)
mse = (sum((test.r-true.pred)^2))/nrow(test);
mse
data.frame(test.r,true.pred)
nn <- neuralnet(medv~tax+nox+ptratio+lstat+rm+indus,data = train,hidden = c(5,3),linear.output =TRUE)
plot(nn)
pred <- compute(nn,subset(test,select = -c(medv)))
str(pred)
scaled.dataf<-scale(dataf,center = mins,scale = maxs-mins);
scaled<- data.frame(scaled.dataf)
set.seed(101)
sample<- sample.split(scaled,SplitRatio = 0.7)
train<- subset(scaled,sample==TRUE)
test<- subset(scaled,sample==FALSE);
library(neuralnet)
nn <- neuralnet(medv~tax+nox+ptratio+lstat+rm+indus,data = train,hidden = c(5,3),linear.output =TRUE)
plot(nn)
pred <- compute(nn,subset(test,select = -c(medv)))
str(pred)
true.pred<- (pred$net.result*(max(dataf$medv)-min(dataf$medv))) + min(dataf$medv)
test.r<- test$medv*(max(dataf$medv)-min(dataf$medv)) + min(dataf$medv)
mse = (sum((test.r-true.pred)^2))/nrow(test);
mse
data.frame(test.r,true.pred)
head(data.frame(test.r,true.pred))
pl+geom_point(aes(age))
pl+geom_point(aes(rad))
pl+geom_point(aes(lstat))
pl+geom_point(aes(black))
nn <- neuralnet(medv~tax+nox+ptratio+lstat+rm+age+indus,data = train,hidden = c(5,3),linear.output =TRUE)
plot(nn)
pred <- compute(nn,subset(test,select = -c(medv)))
str(pred)
true.pred<- (pred$net.result*(max(dataf$medv)-min(dataf$medv))) + min(dataf$medv)
test.r<- test$medv*(max(dataf$medv)-min(dataf$medv)) + min(dataf$medv)
mse = (sum((test.r-true.pred)^2))/nrow(test);
mse
head(data.frame(test.r,true.pred))
plot(nn)
err.df<-data.frame(test.r,true.pred)
ggplot(err.df)+geom_point(aes(test.r,true.pred))+geom_smooth(aes(test.r,true.pred))
setwd("D:/pvsc/data.sci/DATA-ANALYTICS/R/bank-authentication")
dataf<- read.delim('data_banknote_authentication.txt',sep=',')
dataf<- read.delim('data_banknote_authentication.txt',sep=',')
dataf<- read.csv('data_banknote_authentication.txt',sep=',')
dataf<- read.csv('data_banknote_authentication.txt',sep=',',header = T,row.names = NULL)
head(dataf)
corrgram(dataf)
dataf<- read.csv('data_banknote_authentication.txt',sep=',',header = T,row.names = NULL)
head(dataf)
corrgram(dataf)
cor(dataf)
pl<- ggplot(dataf,aes(y=X.Class.))
pl+geom_point(aes(X.Image.Var.))
ggplot(dataf,aes(X.Image.Var.))+geom_point(aes(color=X.Class.))
id <- seq(1:nrow(dataf))
dataf<-cbind(id,dataf)
head(dataf)
ggplot(dataf,aes(X.Image.Var.))+geom_point(aes(x=id,color=X.Class.))
ggplot(dataf,aes(y=X.Image.Var.))+geom_point(aes(x=id,color=X.Class.))
ggplot(dataf)+geom_bar(aes(X.Image.Var.,fill=factor(X.Class.)),position = 'dodge')
ggplot(dataf)+geom_bar(aes(X.Class.,fill=factor(X.Class.)),position = 'dodge')
ggplot(dataf,aes(y=X.Image.Skew.))+geom_point(aes(x=id,color=X.Class.))
ggplot(dataf,aes(y=X.Image.Curt.))+geom_point(aes(x=id,color=X.Class.))
ggplot(dataf,aes(y=X.Entropy.))+geom_point(aes(x=id,color=X.Class.))
#scaling data.
maxs<- sapply(dataf,2,max)
#scaling data.
maxs<- apply(dataf,2,max)
mins<- apply(dataf,2,min)
scaled.df<- scale(dataf,center = maxs, scale= maxs - mins)
head(scaled.df)
dataf<- subset(dataf,select = -c(id))
#scaling data.
maxs<- apply(dataf,2,max)
mins<- apply(dataf,2,min)
scaled.df<- scale(dataf,center = maxs, scale= maxs - mins)
head(scaled.df)
scaled.df<-data.frame(scaled.df)
library(caTools)
set.seed(101)
sample<- sample.split(scaled.df,SplitRatio = 0.7)
train<- subset(scaled.df,sample==TRUE)
test<- subset(scaled.df,sample==FALSE);
head(train)
nn<- neuralnet(X.Class.~ (X.Image.Var. + X.Image.Skew. + X.Image.Curt. + X.Entropy),data = train, hidden =c(4,4),linear.output = F)
nn<- neuralnet(X.Class.~ (X.Image.Var. + X.Image.Skew. + X.Image.Curt. + X.Entropy.),data = train, hidden =c(4,4),linear.output = F)
plot(nn)
pred <- compute(nn, subset(test,select= -c(X.Class.)))
str(pred)
head(pred$net.result)
unique(dataf$X.Class.)
unique(train$X.Class.)
round(pred$net.result[1])
max(pred$net.result)
nn<- neuralnet(X.Class.~ (X.Image.Var. + X.Image.Skew. + X.Image.Curt. + X.Entropy.),data = train, hidden =c(4,4),linear.output = FALSE)
pred <- compute(nn, subset(test,select= -c(X.Class.)))
str(pred)
round(pred$net.result[1])
max(pred$net.result)
nn<- neuralnet(X.Class.~ (X.Image.Var. + X.Image.Skew. + X.Image.Curt. + X.Entropy.),data = train, hidden =10,linear.output = FALSE)
plot(nn)
pred <- compute(nn, subset(test,select= -c(X.Class.)))
str(pred)
round(pred$net.result[1])
max(pred$net.result)
head(pred$net.result)
ans <- sapply(pred$net.result,round)
unique(ans)
table(ans,test$X.Class.)
nn<- neuralnet(X.Class.~ X.Image.Var. + X.Image.Skew. + X.Image.Curt. + X.Entropy. ,data = train, hidden =10,linear.output = FALSE)
plot(nn)
pred <- compute(nn, subset(test,select= -c(X.Class.)))
str(pred)
round(pred$net.result[1])
max(pred$net.result)
temp < -1.928034e-05
temp <- 1.928034e-05
round(temp)
round(temp,5)
round(temp,4)
round(temp,6)
round(temp,5)
#scaling data.
maxs<- apply(dataf,2,max)
mins<- apply(dataf,2,min)
scaled.df<- scale(subset(dataf,select= -c(X.Class.)),center = maxs, scale= maxs - mins)
head(scaled.df)
#scaling data.
maxs<- apply(subset(dataf,select= -c(X.Class.)),2,max)
mins<- apply(subset(dataf,select= -c(X.Class.)),2,min)
scaled.df<- scale(subset(dataf,select= -c(X.Class.)),center = maxs, scale= maxs - mins)
head(scaled.df)
scaled.df<-data.frame(scaled.df,X.Class.)
head(scaled.df)
scaled.df<-data.frame(scaled.df,dataf$X.Class.)
head(scaled.df)
library(caTools)
set.seed(101)
sample<- sample.split(scaled.df,SplitRatio = 0.7)
train<- subset(scaled.df,sample==TRUE);
test<- subset(scaled.df,sample==FALSE);
nn<- neuralnet(X.Class.~ X.Image.Var. + X.Image.Skew. + X.Image.Curt. + X.Entropy. ,data = train, hidden =10,linear.output = FALSE)
nn<- neuralnet(dataf.X.Class.~ X.Image.Var. + X.Image.Skew. + X.Image.Curt. + X.Entropy. ,data = train, hidden =10,linear.output = FALSE)
plot(nn)
pred <- compute(nn, subset(test,select= -c(dataf.X.Class.)))
str(pred)
round(pred$net.result[1])
max(pred$net.result)
head(pred$net.result)
ans <- sapply(pred$net.result,round)
unique(ans)
table(ans,test$dataf.X.Class.)
plot(nn)
